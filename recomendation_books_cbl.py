# -*- coding: utf-8 -*-
"""recomendation-books-CBL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ecKi_ZbRkjDkhvo72jAAs2Is4uRXHN2M

# **Sistem Rekomendasi Buku**

Proyek ini merupakan bagian dari submission kelas Machine Learning Terapan di Dicoding. Tujuan utama dari proyek ini adalah membangun sistem rekomendasi dengan memilih salah satu pendekatan yaitu, ***Content Based Filtering*** atau ***Collaborative Filtering*** sebagai algoritma utama dalam mengatasi masalah yang dipilih.


<br>Pada proyek ini, sistem rekomendasi buku dibangun dengan menggunakan pendekatan ***Collaborative Filtering*** sebagai algoritma utama dalam membangun model rekomendasi.

### ğŸ“š Dataset yang Digunakan

Dataset yang digunakan adalah dataset buku dari Amazon yang tersedia di [Kaggle](https://www.kaggle.com/datasets/saurabhbagchi/books-dataset). Dataset ini terdiri dari tiga file berformat `.csv`, yaitu:

- **`books.csv`**  
  Berisi informasi tentang buku, terdiri dari 8 kolom:
  - `ISBN`
  - `Book-Title`
  - `Book-Author`
  - `Year-Of-Publication`
  - `Publisher`
  - `Image-URL-S`
  - `Image-URL-M`
  - `Image-URL-L`

- **`ratings.csv`**  
  Berisi data penilaian buku oleh pengguna, terdiri dari 3 kolom:
  - `User-ID`
  - `ISBN`
  - `Book-Rating`

- **`users.csv`**  
  Berisi informasi pengguna, terdiri dari 3 kolom:
  - `User-ID`
  - `Location`
  - `Age`

## **Import Library**

Pada bagian ini, saya mengimpor seluruh library Python yang akan digunakan selama proyek ini berlangsung:
"""

import pandas as pd
from pathlib import Path
import numpy as np
import re

"""## **Data Understanding**

### ğŸ“‚ Pembacaan Dataset

Pada tahap ini, dataset dibaca dari Google Drive dengan menggunakan `pandas`. Dataset terdiri dari tiga file utama:

- `books.csv`: berisi informasi detail buku
- `users.csv`: berisi data pengguna
- `ratings.csv`: berisi data rating atau penilaian buku oleh pengguna
"""

# Dataset path
book_path = Path('/content/drive/MyDrive/Books-Ratings-Dataset/books.csv')
user_path = Path('/content/drive/MyDrive/Books-Ratings-Dataset/users.csv')
rating_path = Path('/content/drive/MyDrive/Books-Ratings-Dataset/ratings.csv')

books = pd.read_csv(book_path, encoding='latin1', sep=';', on_bad_lines='warn')
user = pd.read_csv(user_path, encoding='latin1', sep=';', on_bad_lines='warn')
rating = pd.read_csv(rating_path, encoding='latin1', sep=';', on_bad_lines='warn')

"""âš ï¸ Peringatan dan Penanganannya
Selama proses pembacaan dataset, muncul beberapa peringatan (warnings) sebagai berikut:

- ParserWarning: Terjadi karena beberapa baris pada file books.csv memiliki jumlah kolom yang tidak sesuai dengan header (misalnya seharusnya 8 kolom, tapi ditemukan 9 atau lebih). Hal ini bisa disebabkan oleh adanya tanda pemisah (delimiter ;) tambahan di dalam teks judul atau nama penerbit buku.

Contoh pesan:

```
Skipping line 6452: expected 8 fields, saw 9
```

Untuk menghindari error, digunakan parameter on_bad_lines='warn' agar baris yang bermasalah dilewati, namun tetap memberikan peringatan.

- DtypeWarning: Kolom Year-Of-Publication terdeteksi memiliki tipe data campuran, misalnya angka dan teks (seperti 'unknown'). Hal ini dapat menyebabkan pandas kesulitan menetapkan tipe data yang konsisten.

### **ğŸ” Mengecek Struktur Kolom Data**

Untuk memahami struktur masing-masing dataset, digunakan fungsi `DataFrame.info()` pada ketiga dataset utama, yaitu `books`, `user`, dan `rating`.

- **`books.info()`** digunakan untuk melihat informasi struktur dataset buku. Hasilnya menunjukkan jumlah entri (baris), nama kolom, jumlah data non-null per kolom, tipe data masing-masing kolom, serta penggunaan memori. Informasi ini penting untuk mengetahui apakah terdapat kolom dengan data yang hilang atau tidak lengkap.

- **`user.info()`** memberikan gambaran tentang struktur data pengguna. Dari hasilnya terlihat bahwa kolom `Age` memiliki nilai null (kosong) cukup banyak, menunjukkan bahwa sebagian besar pengguna tidak mencantumkan usia. Hal ini penting untuk diperhatikan dalam tahap pembersihan data.

- **`rating.info()`** menampilkan informasi tentang data penilaian pengguna terhadap buku. Dataset ini biasanya terdiri dari kolom `User-ID`, `ISBN`, dan `Book-Rating`. Melalui fungsi ini, kita bisa memastikan tidak ada kolom yang hilang dan format data sudah sesuai.

Secara keseluruhan, fungsi `.info()` membantu mengidentifikasi kualitas dan kesiapan data sebelum dilakukan analisis lebih lanjut atau pelatihan model.

âœ… books.info()
"""

books.info()

"""ğŸ“Œ Penjelasan:

- Dataset books memiliki 271.360 baris dan 8 kolom.

- Hampir semua kolom terisi penuh, namun ada sedikit nilai kosong di kolom Book-Author dan Publisher.

- Semua kolom bertipe object (teks), termasuk Year-Of-Publication yang seharusnya bertipe numerik, sehingga perlu diperiksa atau dikonversi.

âœ… user.info()
"""

user.info()

"""ğŸ“Œ Penjelasan:

- Dataset user memiliki 278.858 baris dan 3 kolom.

- Kolom User-ID dan Location lengkap, namun kolom Age memiliki banyak nilai kosong (sekitar 110.000 baris).

- Kolom Age bertipe float64, menunjukkan ada kemungkinan data usia tidak bulat atau mengandung nilai tidak valid (seperti 0, NaN, atau usia ekstrem).

âœ… rating.info()
"""

rating.info()

"""ğŸ“Œ Penjelasan:

- Dataset rating memiliki 1.149.780 baris, menandakan aktivitas penilaian yang sangat besar.

- Semua kolom lengkap (tidak ada nilai kosong).

- Kolom Book-Rating bertipe integer, dengan nilai rating dari pengguna terhadap buku tertentu.

### Membaca jumlah data
"""

print('Jumlah data buku ', len(books.ISBN.unique()))
print('Jumlah data profil pengguna: ', len(user['User-ID'].unique()))
print('Jumlah data penilaian yang diberikan pengguna: ', len(rating['User-ID'].unique()))
print('Jumlah data penilaian buku: ', len(rating.ISBN.unique()))

"""ğŸ“Œ Penjelasan:
- Jumlah data buku (ISBN) sebanyak 271.360 menandakan jumlah total buku unik dalam dataset books.

- Jumlah profil pengguna (User-ID) sebanyak 278.858 menunjukkan banyaknya pengguna yang terdaftar, meskipun tidak semuanya memberikan rating.

- Jumlah pengguna yang memberikan penilaian sebanyak 105.283, artinya hanya sebagian pengguna yang aktif memberikan rating terhadap buku.

- Jumlah buku yang dinilai (ISBN dalam dataset rating) sebanyak 340.556, artinya jumlah kombinasi penilaian terhadap buku lebih besar dari jumlah buku yang tercatat di dataset books.

  Hal ini bisa disebabkan oleh:
  - Adanya buku-buku yang hanya muncul di `ratings.csv` namun tidak tersedia di `books.csv`.
  - Duplikasi `ISBN` pada buku yang sama dengan format atau versi berbeda.
  - Kesalahan input data dari pengguna (misalnya salah ketik ISBN).

## **Univariate Exploratory Data Analysis** & **Data Preprocessing**

### ğŸ‘¤ **Users DataFrame**

#### 1. Menangani Nilai Kosong pada Kolom `Age`

Salah satu variabel penting dalam data pengguna (`users.csv`) adalah `Age`. Namun, kolom ini memiliki banyak nilai kosong dan outlier. Maka dari itu, kita perlu melakukan pembersihan data dengan tahapan sebagai berikut:

- Menampilkan nilai unik pada kolom `Age`.
- Mengidentifikasi nilai yang tidak logis, seperti usia di bawah 5 atau di atas 90 tahun.
- Mengganti nilai-nilai tersebut menjadi `NaN`.
- Mengisi nilai `NaN` dengan rata-rata usia.
- Mengubah tipe data kolom `Age` ke `int`.

Langkah-langkah ini bertujuan untuk memastikan kualitas data lebih baik sebelum digunakan dalam proses pemodelan.
"""

# Cek nilai unik dari kolom Age
print("Nilai unik Age (sebelum dibersihkan):")
print(sorted(user['Age'].unique()))

# Cek jumlah baris total
print("Jumlah baris total:", len(user))

# Cek jumlah NaN awal
print("Jumlah nilai kosong (NaN) pada kolom Age sebelum pembersihan:", user['Age'].isna().sum())

"""### âš ï¸ Mengganti Nilai Usia yang Tidak Realistis

Beberapa data pada kolom `Age` berisi nilai ekstrem, seperti di bawah 5 tahun atau di atas 90 tahun, yang dianggap tidak realistis. Oleh karena itu, kita ubah nilai tersebut menjadi `NaN` agar dapat ditangani dengan teknik imputasi.
"""

# Salin DataFrame asli ke versi baru agar tidak mengubah data mentah
user_cleaned = user.copy()

# Ganti nilai usia yang tidak realistis dengan NaN
user_cleaned.loc[(user_cleaned['Age'] < 5) | (user_cleaned['Age'] > 90), 'Age'] = np.nan

"""### ğŸ“Š Menghitung Rata-rata Usia Tanpa Outlier

Setelah mengganti nilai tidak realistis dengan `NaN`, kita hitung rata-rata dari kolom `Age` untuk digunakan pada proses imputasi (pengisian nilai kosong).
"""

# Hitung rata-rata usia (tanpa outlier)
mean_age = user_cleaned['Age'].mean()
print("Rata-rata usia (tanpa outlier):", mean_age)

"""### ğŸ”§ Imputasi Nilai Kosong dengan Rata-rata

Nilai `NaN` pada kolom `Age` diisi (imputasi) menggunakan nilai rata-rata usia yang telah dihitung sebelumnya. Ini adalah metode umum dalam menangani data numerik yang hilang
"""

# Imputasi nilai NaN dengan mean
user_cleaned['Age'].fillna(mean_age, inplace=True)

"""### ğŸ”¢ Mengubah Tipe Data ke Integer

Setelah nilai usia dibersihkan dan tidak lagi mengandung nilai kosong, kita ubah tipe data dari float ke integer (`int`). Hal ini dilakukan agar kolom `Age` lebih sesuai untuk interpretasi dan efisien dalam penyimpanan memori.

"""

# Ubah tipe data Age ke integer
user_cleaned['Age'] = user_cleaned['Age'].astype(int)

# Tampilkan nilai unik setelah dibersihkan
print("Nilai unik Age (setelah dibersihkan):")
print(sorted(user_cleaned['Age'].unique()))

user_cleaned.info()

"""Data user sekarang sudah bersih

### ğŸ“š **Books DataFrame**
"""

books.info()

"""#### **Cek Nilai Kosong**"""

# Cek jumlah nilai kosong di setiap kolom
books.isnull().sum()

# Salin DataFrame agar data mentah tidak berubah
books_cleaned = books.copy()

"""Kolom `Image-URL-S`, `Image-URL-M`, dan `Image-URL-L` berisi URL gambar buku yang tidak digunakan dalam proses analisis atau rekomendasi, sehingga dapat dihapus untuk mengurangi kompleksitas data."""

# Drop kolom gambar karena tidak digunakan dalam sistem rekomendasi
books_cleaned.drop(['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], axis=1, inplace=True)

"""Selanjutnya mengecek kolom `Book-Author` yang mempunyai *missing value*"""

# ğŸ” Cek baris dengan Book-Author yang kosong
missing_authors = books_cleaned[books_cleaned['Book-Author'].isnull()]
print("Baris dengan Book-Author kosong:")
print(missing_authors)

"""Mengisi `Book-Author` yang kosong dengan mencari baris lain yang memiliki ISBN yang sama jika tidak ditemukan mencari data dari Amazon. Jika tidak ditemukan juga, nilai diisi dengan `'Unknown'`."""

# âœï¸ Isi nilai kosong berdasarkan ISBN (jika ISBN sama dengan baris lain)
for idx in missing_authors.index:
    isbn = books_cleaned.loc[idx, 'ISBN']

    # Cari baris lain dengan ISBN sama dan author tidak kosong
    same_isbn = books_cleaned[(books_cleaned['ISBN'] == isbn) & (books_cleaned['Book-Author'].notnull())]

    if not same_isbn.empty:
        books_cleaned.loc[idx, 'Book-Author'] = same_isbn.iloc[0]['Book-Author']
    else:
        books_cleaned.loc[idx, 'Book-Author'] = 'Unknown'

# âœ… Cek ulang apakah masih ada Publisher yang kosong
books_cleaned['Book-Author'].isnull().sum()

"""Kolom `Publisher`"""

# ğŸ” Cek baris dengan Publisher yang kosong
missing_publisher = books_cleaned[books_cleaned['Publisher'].isnull()]
print("Baris dengan Publisher kosong:")
print(missing_publisher)

"""Mengisi kolom `Publisher`, dengan mencari data dari Amazon.

Dari hasilnya, diketahui bahwa terdapat dua ISBN yaitu:
- `193169656X` â†’ Penerbit: **NovelBooks, Inc.**
- `1931696993` â†’ Penerbit: **CreateSpace Independent Publishing Platform**
"""

# âœï¸ Isi data Publisher berdasarkan ISBN hasil pencarian manual (misal dari Amazon)
books_cleaned.loc[books_cleaned['ISBN'] == '193169656X', 'Publisher'] = 'NovelBooks, Inc.'
books_cleaned.loc[books_cleaned['ISBN'] == '1931696993', 'Publisher'] = 'CreateSpace Independent Publishing Platform'

"""Kedua baris tersebut diisi secara manual berdasarkan hasil pencarian ISBN dari situs eksternal (seperti Amazon). Kita gunakan metode `.loc[]` untuk menyasar ISBN tertentu dan mengisi kolom `Publisher` yang kosong dengan data yang benar.

Mengecek kembali data Publisher yang kosong
"""

# âœ… Cek ulang apakah masih ada Publisher yang kosong
books_cleaned['Publisher'].isnull().sum()

"""### **Cek kembali nilai yang kosong**"""

# Cek jumlah nilai kosong di setiap kolom
books_cleaned.isnull().sum()

"""### **ğŸ“… Pembersihan Kolom Year-Of-Publication**"""

books_cleaned.info()

"""Kolom `Year-Of-Publication` saat ini masih bertipe `object` (teks), padahal seharusnya berupa angka. Beberapa nilai bahkan mengandung teks seperti nama penerbit. Disini saya akan:

1. Menyaring data yang valid (angka).
2. Mengubah nilai tahun ke tipe `int`.
3. Mengganti nilai tidak valid (misalnya tahun < 1000 atau > 2025) dengan NaN, lalu imputasi menggunakan modus (tahun terbanyak).

### **ğŸ”„ Mengubah kolom Year-Of-Publication**
"""

# ğŸ”„ Mengubah kolom Year-Of-Publication menjadi tipe data numerik (int), dengan error handling
books_cleaned['Year-Of-Publication'] = pd.to_numeric(books_cleaned['Year-Of-Publication'], errors='coerce')

"""Saya menggunakan `pd.to_numeric()` untuk mengubah kolom `Year-Of-Publication` menjadi angka (`int64` atau `float64`).  
Parameter `errors='coerce'` akan mengubah data yang tidak bisa dikonversi menjadi `NaN` (contohnya tahun seperti `'DK Publishing Inc'` atau `'Gallimard'` yang kadang muncul karena kesalahan input).

### **ğŸ” Cek nilai unik yang tidak realistis**
"""

# ğŸ” Cek nilai unik yang tidak realistis (misalnya sebelum tahun 1000 atau setelah 2025)
print("Tahun yang tidak realistis:")
print(sorted(books_cleaned['Year-Of-Publication'].unique())[:10])  # Tahun terendah
print(sorted(books_cleaned['Year-Of-Publication'].unique())[-10:]) # Tahun tertinggi

"""### **ğŸ§¼ Bersihkan tahun yang tidak valid**"""

# ğŸ§¼ Bersihkan tahun yang tidak valid: set jadi NaN jika < 1000 atau > 2025
books_cleaned.loc[(books_cleaned['Year-Of-Publication'] < 1000) | (books_cleaned['Year-Of-Publication'] > 2025), 'Year-Of-Publication'] = np.nan

"""Tahun terbit buku yang valid umumnya berada antara 1000 dan 2025. Tahun di luar rentang ini dianggap tidak masuk akal dan diubah menjadi `NaN` agar bisa diisi nanti (misalnya dengan median).

### **ğŸ§® Isi nilai NaN dengan median tahun yang valid**
"""

# ğŸ§® Isi nilai NaN dengan median tahun yang valid
median_year = books_cleaned['Year-Of-Publication'].median()
books_cleaned['Year-Of-Publication'].fillna(median_year, inplace=True)

# Ubah ke integer
books_cleaned['Year-Of-Publication'] = books_cleaned['Year-Of-Publication'].astype(int)

"""Setelah tahun tidak valid diubah menjadi `NaN`, kita isi nilai kosong tersebut dengan **median** agar lebih netral.  
Terakhir, kita ubah kolom menjadi integer untuk efisiensi dan konsistensi.

## Cek Kembali Type Data `Year-Of-Publication`
"""

books_cleaned.info()

"""## **Data Preparation**

### **Menyatukan Dataset (Merge DataFrame)**

Gabungkan ketiga data utama:

- ratings.csv

- books_cleaned

- user_cleaned

Tujuannya agar semua informasi yang diperlukan berada dalam satu DataFrame.
"""

# Gabungkan rating dengan books
ratings_books = pd.merge(rating, books_cleaned, on='ISBN', how='inner')

# Gabungkan dengan user
data_merged = pd.merge(ratings_books, user_cleaned, on='User-ID', how='inner')

# Tampilkan 5 data teratas
data_merged.head()

"""- Saya menggunakan `merge()` untuk menggabungkan DataFrame berdasarkan `ISBN` dan `User-ID`.
- `how='inner'` artinya hanya data yang memiliki kecocokan di kedua dataset yang disertakan.
- Hasilnya adalah satu DataFrame lengkap yang siap diproses untuk sistem rekomendasi.

### **Cek dan Filter Rating yang Bernilai 0**

Karena beberapa dataset rating (termasuk dari Amazon) menyertakan Book-Rating = 0 untuk menandakan bahwa pengguna belum benar-benar memberi rating.
"""

# Cek distribusi rating
data_merged['Book-Rating'].value_counts().sort_index()

"""**ğŸ” Analisis Cepat**:
- Rating 0 mendominasi (lebih dari 60 ribu entri) â€” ini menandakan bahwa sebagian besar data hanyalah interaksi pengguna dengan buku (bukan rating sebenarnya).

- Rating 1â€“10 jauh lebih sedikit dan itulah yang benar-benar mencerminkan penilaian pengguna.
"""

# Filter hanya data dengan rating > 0
data_filtered = data_merged[data_merged['Book-Rating'] > 0]

# Cek distribusi ulang
data_filtered['Book-Rating'].value_counts().sort_index()

"""Saya hanya akan menyimpan data rating **yang valid** (nilai 1â€“10), karena rating = 0 menandakan pengguna *tidak memberikan rating* atau hanya *melihat buku* saja.  
Data ini akan digunakan sebagai dasar model rekomendasi kita.
"""

data_filtered.head()

df.isnull().sum()

print("Jumlah data di setiap kolom:")
print(df.count())

# Menampilkan jumlah total baris pada dataset
print("Jumlah total data:", len(data_filtered))

"""## **ğŸ§¹ Data Preparation untuk Collaborative Filtering**

Mengambil kolom yang dibutuhkan saja
"""

# Ambil hanya kolom yang relevan untuk model collaborative filtering
df = data_filtered[['User-ID', 'ISBN', 'Book-Rating']].copy()

"""Kita hanya memerlukan kolom:

- User-ID â†’ sebagai identitas pengguna

- ISBN â†’ sebagai identitas buku

- Book-Rating â†’ sebagai label atau nilai yang ingin kita prediksi

## Mengubah Nama Kolom
"""

# Ubah nama kolom agar lebih seragam
df.columns = ['userID', 'bookID', 'rating']

"""Mengganti nama kolom menjadi userID, bookID, dan rating agar lebih konsisten dan mudah digunakan di tahap encoding/training.

### Mengecek jumlah data
"""

# Cek jumlah total data
print("Jumlah total data:", len(df))

# Cek jumlah data pada setiap kolom
print("\nJumlah data di setiap kolom:")
print(df.count())

# Cek jumlah user dan buku unik
print("\nJumlah user unik:", df['userID'].nunique())
print("Jumlah buku unik:", df['bookID'].nunique())

# Cek distribusi rating
print("\nDistribusi rating:")
print(df['rating'].value_counts().sort_index())

"""#### ğŸ” Analisis Awal Dataset

Sebelum masuk ke tahap encoding, kita perlu mengecek:
- Jumlah total data (interaksi user dan buku).
- Jumlah data valid per kolom.
- Jumlah user dan buku unik.
- Distribusi rating.

Hal ini penting untuk mengetahui apakah data terlalu besar (perlu sampling) dan untuk menentukan ukuran embedding.

### **Filtering & Sampling**

Filtering & Sampling sangat penting ketika kita berhadapan dengan dataset yang sangat besar
"""

# Hanya ambil rating >= 6 untuk fokus pada interaksi positif
df = df[df['rating'] >= 6]

# Ambil sampel 30% saja agar training tidak terlalu lama
df = df.sample(frac=0.3, random_state=42)

"""## **Encode userID dan bookID menjadi indeks integer**"""

# Encoding userID dan bookID menjadi indeks integer
user_ids = df['userID'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
book_ids = df['bookID'].unique().tolist()
book_to_book_encoded = {x: i for i, x in enumerate(book_ids)}

# Mapping hasil encoding ke dataframe
df['user'] = df['userID'].map(user_to_user_encoded)
df['book'] = df['bookID'].map(book_to_book_encoded)

"""ğŸ“Œ Penjelasan:

Saya melakukan encoding ke dalam angka integer karena layer embedding membutuhkan input berupa angka.

map() digunakan untuk menempelkan hasil encoding ke dataframe.

#### Hitung jumlah user dan buku untuk keperluan emmbbedding
"""

num_users = len(user_to_user_encoded)
num_books = len(book_encoded_to_book)

"""Rating perlu dikonversi ke tipe float32 agar kompatibel dengan TensorFlow saat proses pelatihan."""

df['rating'] = df['rating'].values.astype(np.float32)

"""### **Normalisasi nilai rating ke rentang 0-1**

Menormalkan rating ke dalam range [0,1] sangat membantu model dalam proses pelatihan karena fungsi aktivasi sigmoid menghasilkan output dalam rentang tersebut.
"""

min_rating = df['rating'].min()
max_rating = df['rating'].max()

df['scaled_rating'] = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

"""### **Membagi Data untuk Training dan Validasi**"""

# Gabungkan user dan book ke dalam satu array sebagai input
x = df[['user', 'book']].values

# Output target: scaled rating
y = df['scaled_rating'].values

# Bagi menjadi 80% training dan 20% validasi
train_size = int(0.8 * len(x))
x_train, x_val = x[:train_size], x[train_size:]
y_train, y_val = y[:train_size], y[train_size:]

"""Penjelasan :
- x adalah kombinasi user dan book yang menjadi input model.

- y adalah target label berupa rating yang dinormalisasi.

- Dataset di-shuffle otomatis karena kita menggunakan teknik *slicing* langsung.

### **Proses Training**
"""



"""### **ğŸ§  Model Definition - RecommenderNet**

Model ini bertugas untuk memprediksi rating atau menentukan tingkat kecocokan antara seorang pengguna (user) dan sebuah buku (book) berdasarkan interaksi masa lalu (dalam hal ini: rating yang pernah diberikan oleh user ke buku tertentu).

Model ini menggunakan pendekatan Collaborative Filtering berbasis neural network, artinya:

>Model tidak melihat isi buku, penulis, genre, atau fitur buku lainnya, melainkan hanya melihat hubungan antara pengguna dan buku melalui data rating historis.

Kita akan membuat model rekomendasi berbasis embedding sederhana, yang menghitung skor kecocokan antara user dan buku menggunakan operasi dot product.

ğŸ§  Embedding adalah cara untuk mengubah userID atau bookID yang berupa angka ke dalam bentuk vektor berdimensi rendah (contoh: panjang 50).
Contohnya:

- User A â†’ [0.1, 0.2, 0.8, ..., 0.05]

- Book B â†’ [0.7, 0.3, 0.5, ..., 0.6]

Semakin mirip preferensi user terhadap buku tertentu, maka dot product dari dua vektor ini akan menghasilkan nilai tinggi (mendekati 1).

#### **ğŸ”§ 1. Definisikan Kelas Model**
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_books, embedding_size=50, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(
            num_users, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.book_embedding = layers.Embedding(
            num_books, embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.book_bias = layers.Embedding(num_books, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        book_vector = self.book_embedding(inputs[:, 1])
        book_bias = self.book_bias(inputs[:, 1])

        dot_user_book = tf.reduce_sum(user_vector * book_vector, axis=1, keepdims=True)
        x = dot_user_book + user_bias + book_bias

        return tf.nn.sigmoid(x)

"""Penjelasan:

- Embedding: Mengubah user dan book ID ke vektor laten (feature vector).

- dot_user_book: Dot product antara user dan book embeddings untuk menghitung matching score.

- sigmoid: Mengubah hasil akhir ke skala [0,1].

#### **2. Kompilasi Model**
"""

model = RecommenderNet(num_users, num_books, embedding_size=50)

model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Model ini menggunakan:
- Loss Function: BinaryCrossentropy
Karena target kita (rating yang sudah dinormalisasi) berada dalam skala 0â€“1, fungsi ini cocok untuk mengukur perbedaan antara prediksi dan data aktual.

- Optimizer: Adam
Optimizer ini secara otomatis menyesuaikan learning rate untuk setiap parameter, dan sering digunakan karena cepat konvergen.

- Metric: RootMeanSquaredError
Digunakan untuk melihat seberapa jauh rata-rata kesalahan prediksi dari nilai yang sebenarnya. Makin kecil, makin baik.

#### **3. Training Model**
"""

history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    batch_size=64,
    epochs=30
)

"""### **Visualisasi Metriks Evaluasi**"""

import matplotlib.pyplot as plt

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""#### **ğŸ“ˆ Interpretasi Hasil**

**Dari grafik yang dihasilkan:**

- Proses training terlihat cukup stabil dan smooth.

- RMSE pada data training terus menurun secara konsisten, menunjukkan bahwa model mampu memahami pola dalam data pelatihan.

- RMSE pada data validasi mulai stagnan setelah beberapa epoch, namun tetap stabil tanpa lonjakan signifikan, menandakan bahwa tidak terjadi overfitting besar.

**Hasil Akhir**:
- RMSE pada data training: Â±0.019

- RMSE pada data validasi: Â±0.310

Nilai error ini tergolong baik mengingat rating yang diprediksi telah dinormalisasi ke skala 0â€“1. Model ini menunjukkan potensi untuk memberikan rekomendasi buku yang cukup relevan bagi pengguna.

### **ğŸ“– Mendapatkan Rekomendasi Buku**

Setelah model dilatih, kita kini dapat menggunakannya untuk memberikan rekomendasi buku kepada pengguna tertentu. Langkah pertama adalah memilih pengguna secara acak, lalu menentukan daftar buku yang belum pernah dibaca atau diberi rating oleh pengguna tersebut.

Mengapa kita perlu menentukan buku yang belum pernah dikunjungi (dibaca)?
Karena sistem rekomendasi bertugas memberikan buku baru yang berpotensi disukai pengguna, bukan buku yang sudah pernah mereka baca/rating sebelumnya.

Untuk itu, kita akan membuat variabel books_not_read sebagai daftar kandidat buku yang akan direkomendasikan kepada user.

#### **Mendapatkan Rekomendasi**
"""

# Gunakan dataframe df dan book_df (berisi detail buku)
book_df = data_filtered[['ISBN', 'Book-Title', 'Book-Author']].drop_duplicates()
book_df = book_df.rename(columns={'ISBN': 'bookID'})

# Ambil user secara acak
user_id = df.userID.sample(1).iloc[0]

# Daftar buku yang sudah diberi rating oleh user tersebut
books_read_by_user = df[df.userID == user_id]

# Buku yang belum pernah dirating user
books_not_read = book_df[~book_df['bookID'].isin(books_read_by_user.bookID.values)]['bookID']
books_not_read = list(set(books_not_read).intersection(set(book_to_book_encoded.keys())))

# Encode buku-buku tersebut
books_not_read_encoded = [[book_to_book_encoded.get(book)] for book in books_not_read]

# Encode user ID
user_encoder = user_to_user_encoded.get(user_id)

# Bentuk pasangan (user, book)
user_book_array = np.hstack(([[user_encoder]] * len(books_not_read_encoded), books_not_read_encoded))

# Prediksi rating
ratings = model.predict(user_book_array).flatten()

# Ambil 10 rekomendasi teratas
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [book_encoded_to_book.get(books_not_read_encoded[x][0]) for x in top_ratings_indices]

"""#### **ğŸ“‹ Menampilkan Hasil Rekomendasi**"""

print('ğŸ“š Menampilkan Rekomendasi untuk User ID:', user_id)
print('=' * 35)
print('ğŸ“˜ Buku yang sebelumnya diberi rating tinggi:')
print('-' * 35)

# Tampilkan 5 buku dengan rating tertinggi dari user
top_books_user = (
    books_read_by_user.sort_values(by='rating', ascending=False)
    .head(5)
    .bookID.values
)

book_df_rows = book_df[book_df['bookID'].isin(top_books_user)]
for row in book_df_rows.itertuples():
    print(f'{row._2} - oleh {row._3}')

print('-' * 35)
print('ğŸ“— 10 Rekomendasi Buku Terbaik:')
print('-' * 35)

recommended_books = book_df[book_df['bookID'].isin(recommended_book_ids)]
for row in recommended_books.itertuples():
    print(f'{row._2} - oleh {row._3}')

"""ğŸ“ Penjelasan
- Kita memilih 1 pengguna secara acak dari dataset.

- Daftar buku yang belum pernah dirating oleh user disaring menggunakan ~isin().

- Model digunakan untuk memprediksi seberapa besar kemungkinan user menyukai buku-buku yang belum dibaca.

- 10 buku dengan skor tertinggi direkomendasikan sebagai hasil akhir.

- Hasilnya ditampilkan dengan mencantumkan judul buku dan nama penulisnya.
"""

